#!/usr/bin/env bun
import { generateText } from 'ai'
import { openrouter } from '@openrouter/sdk-v5'
import { ensureApiKey } from '@util/check'
import { models } from '@util/models'
import { readFileSync } from 'fs'
import { join } from 'path'

ensureApiKey()

// Use the first model from models array (GPT-4o Mini with image capabilities)
const testModel = models[0]
const imagePath = join(import.meta.dir, 'sample', 'test.jpg')

async function debugImageUpload() {
  try {
    console.log('ğŸ” Reading image file...')
    const imageBuffer = readFileSync(imagePath)
    console.log('ğŸ” Image buffer size:', imageBuffer.length)
    
    const base64Image = imageBuffer.toString('base64')
    console.log('ğŸ” Base64 length:', base64Image.length)
    console.log('ğŸ” Base64 preview:', base64Image.substring(0, 100) + '...')

    // Create messages in the format that should work
    const messages: any = [
      {
        role: 'user',
        content: [
          { 
            type: 'text', 
            text: 'What do you see in this image? Please describe it briefly.' 
          },
          { 
            type: 'image', 
            image: `data:image/jpeg;base64,${base64Image}`
          }
        ]
      }
    ]

    console.log('ğŸ” Message structure:', JSON.stringify(messages, null, 2))

    console.log('ğŸ” Calling generateText...')
    const result = await generateText({
      model: openrouter(testModel.slug),
      messages: messages,
      maxOutputTokens: 200,
      temperature: 0.1
    })

    console.log('ğŸ” Result:', result.text)
    console.log('ğŸ” Usage:', result.usage)
    
  } catch (error) {
    console.error('ğŸ” Error:', error)
  }
}

if (import.meta.main) {
  debugImageUpload()
}